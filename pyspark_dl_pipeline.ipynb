{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I use PySpark, Keras, and Elephas python libraries to build an end-to-end deep learning pipeline that runs on Spark. Spark is an open-source distributed analytics engine that can process large amounts of data with tremendous speed. PySpark is simply the python API for Spark that allows you to use an easy programming language, like python, and leverage the power of Apache Spark.\n",
    "\n",
    "My interest in putting together this example was to learn and prototype. More specifically, learn more about PySpark pipelines as well as how I could integrate deep learning into the PySpark pipeline. I ran this entire project using Jupyter on my local machine to build a prototype for an upcoming data science project where the data will be massive. Since I work for IBM, I'll take this entire analytics project (Jupyter Notebook) and move it to IBM. This allows me to do my data ingestion, pipelining, training and deployment on a unified platform and on a much larger Spark cluster. Obviously, if you had a real and sizable project or using image data you would NOT do this on your local machine.\n",
    "\n",
    "Overall, I found it not too difficult to put together this prototype or working example so I hope others will find it useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n",
      "Collecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=c625d6cd935badb3dd736d66e6de9a02a68d905b163113ba4a541464c6edb025\n",
      "  Stored in directory: c:\\users\\surya.dev\\appdata\\local\\pip\\cache\\wheels\\1d\\27\\68\\1382001655ef41217e1dd34d59aa777612135379bab64279e9\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (2.9.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (1.47.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (20.9)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (52.0.0.post20210125)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elephas==3.0.0Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading elephas-3.0.0.tar.gz (26 kB)\n",
      "Requirement already satisfied: cython in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from elephas==3.0.0) (0.29.23)\n",
      "Requirement already satisfied: tensorflow!=2.2.*,>=2 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from elephas==3.0.0) (2.9.1)\n",
      "Requirement already satisfied: flask in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from elephas==3.0.0) (1.1.2)\n",
      "Requirement already satisfied: h5py==3.3.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from elephas==3.0.0) (3.3.0)\n",
      "Collecting pyspark<3.2\n",
      "  Downloading pyspark-3.1.3.tar.gz (214.0 MB)\n",
      "Requirement already satisfied: numpy>=1.17.5 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from h5py==3.3.0->elephas==3.0.0) (1.20.1)\n",
      "Collecting py4j==0.10.9\n",
      "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.47.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (2.9.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.6.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (3.7.4.3)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (2.9.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (2.9.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (20.9)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (0.26.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (3.19.4)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (14.0.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow!=2.2.*,>=2->elephas==3.0.0) (0.36.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (2.9.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (3.3.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow!=2.2.*,>=2->elephas==3.0.0) (3.1.1)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from flask->elephas==3.0.0) (1.1.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from flask->elephas==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from flask->elephas==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->flask->elephas==3.0.0) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\surya.dev\\anaconda3\\lib\\site-packages (from packaging->tensorflow!=2.2.*,>=2->elephas==3.0.0) (2.4.7)\n",
      "Building wheels for collected packages: elephas, pyspark\n",
      "  Building wheel for elephas (setup.py): started\n",
      "  Building wheel for elephas (setup.py): finished with status 'done'\n",
      "  Created wheel for elephas: filename=elephas-3.0.0-py3-none-any.whl size=26261 sha256=fe6b112e04a9a02841db0c3d0b0fb4c4f10ce25ae1568635ce129f3d2ce56b3b\n",
      "  Stored in directory: c:\\users\\surya.dev\\appdata\\local\\pip\\cache\\wheels\\d4\\ea\\b7\\5ad17bc2a7134e08d9715156f1407801c4171f5a14af9ab868\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.1.3-py2.py3-none-any.whl size=214463482 sha256=523e12812915e666dbaac20cc5b739d2e30d28225af567108a788114aef76511\n",
      "  Stored in directory: c:\\users\\surya.dev\\appdata\\local\\pip\\cache\\wheels\\d1\\b2\\3a\\d019c9382e6429910cef8ef0aae6a1970bfbbba13562590d70\n",
      "Successfully built elephas pyspark\n",
      "Installing collected packages: py4j, pyspark, elephas\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.2.0\n",
      "    Uninstalling pyspark-3.2.0:\n",
      "      Successfully uninstalled pyspark-3.2.0\n",
      "  Attempting uninstall: elephas\n",
      "    Found existing installation: elephas 3.1.0\n",
      "    Uninstalling elephas-3.1.0:\n",
      "      Successfully uninstalled elephas-3.1.0\n",
      "Successfully installed elephas-3.0.0 py4j-0.10.9 pyspark-3.1.3\n"
     ]
    }
   ],
   "source": [
    "pip install elephas==3.0.0 --user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 1:</b> Import Libraries\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4d7156ee3985>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Elephas for Deep Learning on Spark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0melephas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mElephasEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\elephas\\ml_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStructField\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mArrayType\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "source": [
    "# Spark Session, Pipeline, Functions, and Metrics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import rand\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Keras / Deep Learning\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers, regularizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Elephas for Deep Learning on Spark\n",
    "from elephas.ml_model import ElephasEstimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 2:</b> Start Spark Session\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a version of Spark < 2.x your Spark session code may be slightly different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session\n",
    "# conf = SparkConf().setAppName('Spark DL Tabular Pipeline').setMaster('local[6]')\n",
    "conf = SparkConf().setAppName('Spark DL Tabular Pipeline')\n",
    "sc = SparkContext('local', conf=conf)\n",
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 3:</b> Load and Preview Data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll load the data. The data we'll use comes from a [Kaggle competition](https://www.kaggle.com/janiobachmann/bank-marketing-dataset). It's a typical banking dataset. I use the `inferSchema` parameter here which helps to identify the feature types when loading in the data. Per the [PySpark documentation](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html) this _\"requires one extra pass over the data\"_. Since the bank data I'm loading only has ~11k observations it doesnt take long at all, but it may be worth noting if you have a very large dataset.\n",
    "\n",
    "After we load the data we can see the schema and the various feature types. All our features are either `string` type or `integer`. We then preview the first 5 observations. I'm pretty familair with with Pandas python library so through this example you'll see me use `toPandas()` to convert the spark dataframe to a pandas dataframe and do some manipulations. Not right or wrong, just easier for me.\n",
    "\n",
    "Finally, we'll drop the 2 date columns since we won't be using those in our deep learning model. They could be possibly significant and featurized by I decided to just drop them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data to Spark Dataframe\n",
    "df = sql_context.read.csv('C:/Users/surya.dev/PycharmProjects/pyspark_dl_pipeline-master/pyspark_dl_pipeline-master/bank.csv',\n",
    "                    header=True,\n",
    "                    inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Dataframe (Pandas Preview is Cleaner)\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Unnessary Features (Day and Month)\n",
    "df = df.drop('day', 'month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Dataframe\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 4:</b> Create the Spark Data Pipeline\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the pipeline using PySpark. This essentially takes your data and, per the feature lists you pass, will do the transformations and vectorizing so it is ready for modeling. I referenced the [\"Extracting, transforming and selecting features\"](https://spark.apache.org/docs/latest/ml-features.html) Apache Spark documentation a lot for this pipeline and project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helper function to select from the numeric features which ones to standardize based on the kurtosis or skew of that feature. The current defaults for `upper_skew` and `lower_skew` are just general guidelines (depending where you read), but you can modify the upper and lower skew as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to select features to scale given their skew\n",
    "def select_features_to_scale(df=df, lower_skew=-2, upper_skew=2, dtypes='int32', drop_cols=['']):\n",
    "    \n",
    "    # Empty Selected Feature List for Output\n",
    "    selected_features = []\n",
    "    \n",
    "    # Select Features to Scale based on Inputs ('in32' type, drop 'ID' columns or others, skew bounds)\n",
    "    feature_list = list(df.toPandas().select_dtypes(include=[dtypes]).columns.drop(drop_cols))\n",
    "    \n",
    "    # Loop through 'feature_list' to select features based on Kurtosis / Skew\n",
    "    for feature in feature_list:\n",
    "\n",
    "        if df.toPandas()[feature].kurtosis() < -2 or df.toPandas()[feature].kurtosis() > 2:\n",
    "            \n",
    "            selected_features.append(feature)\n",
    "    \n",
    "    # Return feature list to scale\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get into the actual data pipeline. The feature list selection part can be further enhanced to be more dynamic vs listing out each feature, but for this small dataset I just left it as is with `cat_features`, `num_features`, and `label`. Selecting features by type can be done similar to how I did it in the `select_features_to_scale` helper function using something like this `spark_df.toPandas().select_dtypes(include=['object']).columns)` which would return a list of all the columns in your spark dataframe that are object or string type.\n",
    "\n",
    "The first thing we want to do is create an empty list called `stages`. This will contain each step that the data pipeline needs to to complete all transformations within our pipeline. I print out each step of the stages after the pipeline so you can see the sequential steps from my code to a list.\n",
    "\n",
    "The second part is going to be a basic loop to go through each categorical feature from our list `cat_features` and then index and encode those features using one-hot encoding. `StringIndexer` encodes your categorical feature to a feature index with the highest frequency label (count) as feature index `0` and so on. I will preview the transformed data frame after the pipeline, Step 5, where you can see each feature index created from the categorical features. For more information and a basic example of StringIndexer check the [here](https://spark.apache.org/docs/latest/ml-features.html#stringindexer)\n",
    "\n",
    "Within the loop we also do some one-hot encoding (OHE) using the `OneHotEncoderEstimator`. This function only takes a label index so if you have categorical data (objects or strings) you have to use `StringIndexer` so you can pass a label index to the OHE estimator. One nice thing I found from looking at dozens of examples was that you can chain `StringIndexer` output right into the OHE estimator using `string_indexer.getOutputCol()`. If you have a lot of features to transform you'll want to do some thinking about the names, `OutputCol`, because you can't just overwrite feature names so get creative. We'll append all those pipeline steps within our loop into our pipeline list `stages`.\n",
    "\n",
    "Next we use `StringIndexer` again on our label feature or dependent variable. And then we'll move on to scaling the numeric variables using the `select_features_to_scale` helper function from above. Once that list is selected we'll vectorize those features using `VectorAssembler` and then standardize the features within that vector using `StandardScaler`. Then we append those steps to our ongoing pipeline list `stages`.\n",
    "\n",
    "The last step is just assembling all our features into a single vector. We'll find the numeric features from the list `num_features` that were not scaled by just using the difference between our `unscaled_features` (the name of the selected numeric feature TO scale) list and the original list of numeric features `num_features`. Then we assemble or vectorize all the categorical OHE features and numeric features and add that step to our pipeline `stages`. And finally, we add in the `scaled_features` to our `assembled_inputs` to get a final and single vector of features for our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Pipeline\n",
    "cat_features = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
    "num_features = ['age','balance','duration','campaign','pdays','previous']\n",
    "label = 'deposit'\n",
    "\n",
    "# Pipeline Stages List\n",
    "stages = []\n",
    "\n",
    "# Loop for StringIndexer and OHE for Categorical Variables\n",
    "for features in cat_features:\n",
    "    \n",
    "    # Index Categorical Features\n",
    "    string_indexer = StringIndexer(inputCol=features, outputCol=features + \"_index\")\n",
    "    \n",
    "    # One Hot Encode Categorical Features\n",
    "    encoder = OneHotEncoder(inputCols=[string_indexer.getOutputCol()],\n",
    "                                     outputCols=[features + \"_class_vec\"])\n",
    "    # Append Pipeline Stages\n",
    "    stages += [string_indexer, encoder]\n",
    "    \n",
    "# Index Label Feature\n",
    "label_str_index =  StringIndexer(inputCol=label, outputCol=\"label_index\")\n",
    "\n",
    "# Scale Feature: Select the Features to Scale using helper 'select_features_to_scale' function above and Standardize \n",
    "unscaled_features = select_features_to_scale(df=df, lower_skew=-2, upper_skew=2, dtypes='int32', drop_cols=['id'])\n",
    "\n",
    "unscaled_assembler = VectorAssembler(inputCols=unscaled_features, outputCol=\"unscaled_features\")\n",
    "scaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "stages += [unscaled_assembler, scaler]\n",
    "\n",
    "# Create list of Numeric Features that Are Not Being Scaled\n",
    "num_unscaled_diff_list = list(set(num_features) - set(unscaled_features))\n",
    "\n",
    "# Assemble or Concat the Categorical Features and Numeric Features\n",
    "assembler_inputs = [feature + \"_class_vec\" for feature in cat_features] + num_unscaled_diff_list\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"assembled_inputs\") \n",
    "\n",
    "stages += [label_str_index, assembler]\n",
    "\n",
    "# Assemble Final Training Data of Scaled, Numeric, and Categorical Engineered Features\n",
    "assembler_final = VectorAssembler(inputCols=[\"scaled_features\",\"assembled_inputs\"], outputCol=\"features\")\n",
    "\n",
    "stages += [assembler_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the steps within our pipeline by looking at our `stages` list that we've been sequentially adding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 5:</b> Run Data Through the Spark Pipeline\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the _\"hard\"_ part is over we can simply pipeline the stages and fit our data to the pipeline by using `fit()`. Then we actually transform the data by using `transform`. We can now preview our newly transformed PySpark dataframe with all the original and transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "\n",
    "# Fit Pipeline to Data\n",
    "pipeline_model = pipeline.fit(df)\n",
    "\n",
    "# Transform Data using Fitted Pipeline\n",
    "df_transform = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview Newly Transformed Data\n",
    "df_transform.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structure Type is a PySpark Dataframe\n",
    "type(df_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 6:</b> Final Data Prep before Deep Learning Model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are couple last and quick things we need to do before modeling. First is to create a PySpark dataframe that only contains 2 vectors from the recently transformed dataframe. We only need the: `features` (X) and `label_index` (y) features for modeling. It's easy enough to do with PySpark with the simple `select` statement. Then, just cause, we preview the dataframe.\n",
    "\n",
    "Finally, we want to shuffle our dataframe and then split the data into train and test sets. You always want to shuffle the data prior to modeling to avoid any bias from how the data may be sorted or otherwise organized and specifically shuffling prior to splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only 'features' and 'label_index' for Final Dataframe\n",
    "df_transform_fin = df_transform.select('features','label_index')\n",
    "df_transform_fin.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle Data\n",
    "df_transform_fin = df_transform_fin.orderBy(rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into Train / Test Sets\n",
    "train_data, test_data = df_transform_fin.randomSplit([.8, .2],seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 7:</b> Build a Deep Learning Model\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now build a basic deep learning model using Keras. Keras is described as: _\"a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.\"_ in the [Keras documentation](https://keras.io/). I find Keras to be one of the easiest deep learning APIs for python. Also, I found an extension of Keras that allowed me to do easy distributed deep learning on Spark that could integrate with my PySpark pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to determine the number of classes as well as the number of inputs from our data so we can plug those values into our Keras deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Classes\n",
    "nb_classes = train_data.select(\"label_index\").distinct().count()\n",
    "\n",
    "# Number of Inputs or Input Dimensions\n",
    "input_dim = len(train_data.select(\"features\").first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a basic deep learning model. Using the `model = Sequential()` feature from Keras, it's easy to simply add layers and build a deep learning model the all the desired settings (# of units, dropout %, regularization - l2, activation functions, etc.) I selected the common Adam optimizer and binary cross-entropy since out outcome label is binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Deep Learning Model / Architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_shape=(input_dim,), activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(256, activity_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.3))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is built we can view the architecture. Notice that we went from 30 inputs/parameters to 74,242. The beauty (sometimes laziness :) ) of deep learning is the automatic feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 8:</b> Distributed Deep Learning\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model built, using Keras as our deep learning framework, we want to run that model on Spark to leverage its distributed analytic engine. We do that by using a python library and an extension to Keras called [Elephas](https://github.com/maxpumperla/elephas). Elephas makes it pretty easy to run your Keras models on Apache spark with few lines of configuration. I found Elephas to be easier and more stable to use than the several other libraries I read about and tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do with Elephas is create an estimator similar to some of the PySpark pipeline items above. We can set the optimizer settings right from Keras optimizer function and then pass that to our Elephas estimator. I only explicitly use Adam optimizer with a set learning rate, but you can use any [Keras optimizer](https://keras.io/optimizers/) with their respective parameters (clipnorm, beta_1, beta_2, etc.).\n",
    "\n",
    "Then within the Elephas estimator you specify a variety of items: features column, label column, # of epochs, batch size for training, validation split of your training data, loss function, metric, etc. I just used the settings from an [Elephas example](https://github.com/maxpumperla/elephas/blob/master/examples/ml_pipeline_otto.py) and modified the code slightly.\n",
    "\n",
    "Notice that after we run the estimator the output, `ElephasEstimator_31afcd77fffd`, looks similar to one of our pipeline `stages` list items. This can be passed directly into our PySpark pipeline to fit and transform our data which we'll do in the next step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and Serialize Optimizer\n",
    "optimizer_conf = optimizers.Adam(lr=0.01)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)\n",
    "\n",
    "# Initialize SparkML Estimator and Get Settings\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"features\")\n",
    "estimator.setLabelCol(\"label_index\")\n",
    "estimator.set_keras_model_config(model.to_json())\n",
    "estimator.set_categorical_labels(True)\n",
    "estimator.set_nb_classes(nb_classes)\n",
    "estimator.set_num_workers(1)\n",
    "estimator.set_epochs(25) \n",
    "estimator.set_batch_size(64)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_validation_split(0.10)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"binary_crossentropy\")\n",
    "estimator.set_metrics(['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Step 9:</b> Distributed Deep Learning Pipeline and Results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that are deep learning model is to be run on Spark, using Elephas, we can pipeline line it exactly how we did above using `Pipeline()`. You could append this to our `stages` list and do all of this with one pass with a new dataset now that it's all built out which would be super cool!\n",
    "\n",
    "I created another helper function below called `dl_pipeline_fit_score_results` that takes the deep learning pipeline `dl_pipeline` and then does all the fitting, transforming, and prediction on both the train and test data sets. It also outputs the accuracy for both data sets and their confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Deep Learning Pipeline\n",
    "dl_pipeline = Pipeline(stages=[estimator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_pipeline_fit_score_results(dl_pipeline=dl_pipeline,\n",
    "                                  train_data=train_data,\n",
    "                                  test_data=test_data,\n",
    "                                  label='label_index'):\n",
    "    \n",
    "    fit_dl_pipeline = dl_pipeline.fit(train_data)\n",
    "    pred_train = fit_dl_pipeline.transform(train_data)\n",
    "    pred_test = fit_dl_pipeline.transform(test_data)\n",
    "    \n",
    "    pnl_train = pred_train.select(label, \"prediction\")\n",
    "    pnl_test = pred_test.select(label, \"prediction\")\n",
    "    \n",
    "    pred_and_label_train = pnl_train.rdd.map(lambda row: (row[label], row['prediction']))\n",
    "    pred_and_label_test = pnl_test.rdd.map(lambda row: (row[label], row['prediction']))\n",
    "    \n",
    "    metrics_train = MulticlassMetrics(pred_and_label_train)\n",
    "    metrics_test = MulticlassMetrics(pred_and_label_test)\n",
    "    \n",
    "    print(\"Training Data Accuracy: {}\".format(round(metrics_train.precision(),4)))\n",
    "    print(\"Training Data Confusion Matrix\")\n",
    "    display(pnl_train.crosstab('label_index', 'prediction').toPandas())\n",
    "    \n",
    "    print(\"\\nTest Data Accuracy: {}\".format(round(metrics_test.precision(),4)))\n",
    "    print(\"Test Data Confusion Matrix\")\n",
    "    display(pnl_test.crosstab('label_index', 'prediction').toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our new deep learning pipeline and helper function on both data sets and test our results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_pipeline_fit_score_results(dl_pipeline=dl_pipeline,\n",
    "                              train_data=train_data,\n",
    "                              test_data=test_data,\n",
    "                              label='label_index');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope that this example has been helpful. I know it was for me in learning more about PySpark pipelines and doing deep learning on spark using an easy deep learning framework like Keras. Like I mentioned, I ran all this locally with little to no issue. My main objective was to prototype something for an upcoming project that will contain a massive dataset. Luckily working for IBM allows me to leverage Watson so I'll train and deploy on a large Spark cluster. My hope is that you (and myself) can use this as a template while making few changes to things like the spark session, data, feature selection, and maybe adding or removing some pipeline stages. As always, thanks for reading and good luck on your next project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
